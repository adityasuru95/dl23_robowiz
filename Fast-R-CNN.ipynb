{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Fast-R-CNN: Localization based on bounding box**"
      ],
      "metadata": {
        "id": "OVmGdKrxPvBE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk2zBQt85Pd6",
        "outputId": "db0e9cbe-4dc9-4a17-804e-4e82c52eae70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "from torchvision import utils\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/m2cai16-tool-locations.zip"
      ],
      "metadata": {
        "id": "NSowKgQJKga6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse XML files\n",
        "def parse_xml(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    objects = []\n",
        "    for obj in root.iter('object'):\n",
        "        obj_dict = {\n",
        "            'name': obj.find('name').text,\n",
        "            'bbox': [\n",
        "                int(obj.find('bndbox/xmin').text),\n",
        "                int(obj.find('bndbox/ymin').text),\n",
        "                int(obj.find('bndbox/xmax').text),\n",
        "                int(obj.find('bndbox/ymax').text)\n",
        "            ]\n",
        "        }\n",
        "        objects.append(obj_dict)\n",
        "    return objects\n",
        "\n",
        "# Visual bounding boxes\n",
        "def visualize_image(image_path, objects):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # OpenCV uses BGR by default\n",
        "    for obj in objects:\n",
        "        bbox = obj['bbox']\n",
        "        cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
        "        cv2.putText(image, obj['name'], (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "    plt.imshow(image)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-rOHfEUcAmLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "your_label_map = {\n",
        "    \"Grasper\": 1,\n",
        "    \"Bipolar\": 2,\n",
        "    \"Hook\": 3,\n",
        "    \"Scissors\": 4,\n",
        "    \"Clipper\": 5,\n",
        "    \"Irrigator\": 6,\n",
        "    \"SpecimenBag\": 7\n",
        "}\n",
        "\n",
        "\n",
        "def resize_bbox(bbox, in_size, out_size):\n",
        "    \"\"\"\n",
        "    Adjust bounding box size.\n",
        "    :param bbox: Original bounding box coordinates in the format [xmin, ymin, xmax, ymax]\n",
        "    :param in_size: original image size (width, height)\n",
        "    :param out_size: new image size (width, height)\n",
        "    :return: adjusted bounding box coordinates\n",
        "    \"\"\"\n",
        "    xmin, ymin, xmax, ymax = bbox\n",
        "    scale_x = out_size[0] / in_size[0]\n",
        "    scale_y = out_size[1] / in_size[1]\n",
        "    xmin = int(xmin * scale_x)\n",
        "    ymin = int(ymin * scale_y)\n",
        "    xmax = int(xmax * scale_x)\n",
        "    ymax = int(ymax * scale_y)\n",
        "    return [xmin, ymin, xmax, ymax]\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, imgs_dir, anns_dir, transforms=None):\n",
        "        self.imgs_dir = imgs_dir\n",
        "        self.anns_dir = anns_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # Get the names of all image and annotation files, with extensions removed\n",
        "        anns = set(os.path.splitext(file)[0] for file in os.listdir(anns_dir))\n",
        "        imgs = anns\n",
        "\n",
        "        # Keep image file names with matching annotations\n",
        "        self.imgs = [file + '.jpg' for file in imgs]  # Assuming the image file is in .jpg format\n",
        "        self.annotations = [file + '.xml' for file in anns]  # Assume that the annotation file is in .xml format\n",
        "\n",
        "        # Make sure the image and annotation file lists are of equal length\n",
        "        assert len(self.imgs) == len(self.annotations), \"Images and annotations lists are not the same length\"\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.imgs_dir, self.imgs[idx])\n",
        "        ann_path = os.path.join(self.anns_dir, self.annotations[idx])\n",
        "\n",
        "        # read images\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        original_size = image.size\n",
        "        new_size = (600, 600)\n",
        "\n",
        "        # Parse XML files using parse_xml function\n",
        "        objects = parse_xml(ann_path)\n",
        "\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for obj in objects:\n",
        "            label_name = obj['name']\n",
        "            label = your_label_map[label_name]  # Convert category name to integer\n",
        "            labels.append(label)\n",
        "\n",
        "            # Add bounding box coordinates\n",
        "            bbox = obj['bbox']\n",
        "            resized_bbox = resize_bbox(bbox, original_size, new_size)\n",
        "            boxes.append(resized_bbox)\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)  # Apply conversion to image\n",
        "            # If necessary, you can also convert the target accordingly\n",
        "            # For example, if random flipping is used, the bounding box of the target needs to be adjusted accordingly\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n"
      ],
      "metadata": {
        "id": "7v00m8FHAoan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import functional as F\n",
        "\n",
        "\n",
        "imgs_dir = \"/content/m2cai16-tool-locations/JPEGImages\"\n",
        "anns_dir = \"/content/m2cai16-tool-locations/Annotations\"\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # Convert PIL image to PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    # if train:\n",
        "    #     # Data augmentation during training phase, such as random horizontal flipping\n",
        "    #     transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    #     # More data enhancement operations can be added, such as:\n",
        "    #     # transforms.append(T.RandomVerticalFlip(0.5))\n",
        "    #     # transforms.append(T.ColorJitter(...))\n",
        "    transforms.append(T.Resize((600, 600)))  # 以 600x600 为例\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Stack or pack images and targets separately, because images have different numbers of boxes\n",
        "    images = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]\n",
        "\n",
        "    images = default_collate(images)  # Use default stacking method for images\n",
        "    # Targets do not need to be stacked as they may be of different lengths\n",
        "    return images, targets\n",
        "\n",
        "\n",
        "# Create a dataset instance\n",
        "dataset = CustomDataset(imgs_dir, anns_dir, transforms=get_transform(True))\n",
        "data_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "3Kdr_KfqF1cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[8][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHsAgxkvF9MT",
        "outputId": "8924e7bf-ff3f-48c5-b465-f739bdbc466f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 600, 600])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(dataset))[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQhTNsO9Y0NH",
        "outputId": "17d47b16-62d0-459e-f653-52a17d58225f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': tensor([[143., 474., 254., 589.]]), 'labels': tensor([6])}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# load pre-trained model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Get the number of input features of the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# Replace the head of a pretrained model to adapt to the number of classes in your dataset\n",
        "# Assume your dataset has num_classes classes (including background)\n",
        "num_classes = 8  # Adjust based on the number of categories in your dataset\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LP8FqwGAfuE",
        "outputId": "c066f519-8b14-49e4-cad0-33a579ca72ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:01<00:00, 90.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# choose optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# training epoch\n",
        "num_epochs = 6  # adjust according to needs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, targets in data_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch} Loss: {losses.item()}\")\n"
      ],
      "metadata": {
        "id": "UWiEtNLdF5dl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "88dc2d8c-70c3-48a3-c09a-045ed40a7fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 0.1141739934682846\n",
            "Epoch 1 Loss: 0.2700803577899933\n",
            "Epoch 2 Loss: 0.13415004312992096\n",
            "Epoch 3 Loss: 0.30601173639297485\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d9f5aaa414b2>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d9f5aaa414b2>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}